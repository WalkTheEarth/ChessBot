# üß† ChessBot - AI Chess Engine Trainer

A sophisticated reinforcement learning system that trains neural networks to play chess using Deep Q-Learning, self-play, and engine opposition. Watch your AI evolve from random moves to strategic masterpieces!

## üéØ Key Features

### üß† **Advanced Neural Architecture**
- **Deep Q-Network (DQN)** with 3-layer CNN for spatial pattern recognition
- **12-channel board representation** (6 piece types √ó 2 colors)
- **Experience replay** with 10,000-move memory buffer
- **Target network** for stable training convergence

### ‚ö° **High-Performance Training**
- **Multi-threaded execution** - Up to 8x speedup with parallel games
- **Batch move evaluation** - Evaluates all legal moves simultaneously
- **GPU acceleration** - Automatic CUDA detection and utilization
- **Thread-safe operations** - Concurrent training without data corruption

### üéÆ **Flexible Training Modes**
- **Self-play training** - AI learns by playing against itself
- **Engine opposition** - Train against Stockfish at configurable difficulty (0-20)
- **Mixed training** - Combine self-play and engine games for balanced learning
- **Progressive difficulty** - Gradually increase opponent strength

### üìä **Real-Time Monitoring**
- **Live statistics** - Win rate, games/sec, ETA, epsilon decay
- **Automatic checkpointing** - Save progress every N games
- **Rich terminal UI** - Beautiful progress displays with Rich library
- **Training analytics** - Detailed performance metrics and trends

## üìã System Requirements

### **Core Dependencies**
- **Python 3.8+** (3.9+ recommended for best performance)
- **PyTorch 1.9+** (with CUDA support for GPU acceleration)
- **python-chess 1.9+** (chess library and UCI protocol)
- **NumPy 1.21+** (numerical computations)
- **Rich 10.0+** (beautiful terminal UI)

### **Optional Dependencies**
- **Stockfish** (chess engine for opposition training)
- **CUDA Toolkit** (for GPU acceleration)
- **Visual Studio Build Tools** (Windows, for PyTorch compilation)

### **Hardware Recommendations**
- **CPU**: 4+ cores (8+ cores for optimal multi-threading)
- **RAM**: 8GB+ (16GB+ for large batch sizes)
- **GPU**: NVIDIA GPU with 4GB+ VRAM (optional but recommended)
- **Storage**: 2GB+ free space for models and checkpoints

## üöÄ Quick Start Installation

### **Step 1: Clone the Repository**
```bash
git clone https://github.com/yourusername/ChessBot.git
cd ChessBot
```

### **Step 2: Create Virtual Environment (Recommended)**
```bash
# Create virtual environment
python -m venv chessbot_env

# Activate environment
# Windows:
chessbot_env\Scripts\activate
# Linux/macOS:
source chessbot_env/bin/activate
```

### **Step 3: Install Dependencies**
```bash
# Install core dependencies
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118
pip install numpy python-chess rich stockfish

# Or install all at once
pip install -r requirements.txt  # (if you create this file)
```

### **Step 4: Install Stockfish (Optional but Recommended)**

**Windows:**
```bash
# Download from https://stockfishchess.org/download/
# Extract to C:\Stockfish\ and note the path to stockfish.exe
```

**Linux (Ubuntu/Debian):**
```bash
sudo apt update
sudo apt install stockfish
```

**macOS:**
```bash
brew install stockfish
```

**Verify Installation:**
```bash
python -c "import torch; print(f'PyTorch: {torch.__version__}'); print(f'CUDA: {torch.cuda.is_available()}')"
```

## üìÅ Project Architecture

```
ChessBot/
‚îú‚îÄ‚îÄ üß† Core AI Components
‚îÇ   ‚îú‚îÄ‚îÄ agent.py          # DQN agent with experience replay
‚îÇ   ‚îú‚îÄ‚îÄ model.py          # CNN neural network architecture
‚îÇ   ‚îî‚îÄ‚îÄ chess_env.py      # Chess environment & state representation
‚îÇ
‚îú‚îÄ‚îÄ üéÆ Training & Execution
‚îÇ   ‚îú‚îÄ‚îÄ main.py           # Multi-threaded CLI training interface
‚îÇ   ‚îú‚îÄ‚îÄ train.py          # Training loop and game simulation
‚îÇ   ‚îî‚îÄ‚îÄ uci_handler.py    # Stockfish/UCI engine integration
‚îÇ
‚îú‚îÄ‚îÄ üñ•Ô∏è User Interface
‚îÇ   ‚îî‚îÄ‚îÄ tui.py            # Rich terminal UI (experimental)
‚îÇ
‚îú‚îÄ‚îÄ üíæ Model Storage
‚îÇ   ‚îú‚îÄ‚îÄ checkpoint_*.pth  # Training checkpoints
‚îÇ   ‚îú‚îÄ‚îÄ final_model.pth   # Final trained model
‚îÇ   ‚îî‚îÄ‚îÄ Model1/, Model2/, Model3/  # Alternative model versions
‚îÇ
‚îî‚îÄ‚îÄ üìö Documentation
    ‚îî‚îÄ‚îÄ README.md         # This comprehensive guide
```

### **Key Components Explained**

| Component | Purpose | Key Features |
|-----------|---------|--------------|
| `agent.py` | DQN implementation | Experience replay, epsilon-greedy, target network |
| `model.py` | Neural architecture | 3-layer CNN, 12-channel input, dropout regularization |
| `chess_env.py` | Game environment | Board state encoding, move validation, reward calculation |
| `main.py` | Training interface | Multi-threading, CLI arguments, progress monitoring |
| `train.py` | Training logic | Game simulation, opponent selection, statistics tracking |
| `uci_handler.py` | Engine interface | Stockfish integration, skill level configuration |

## üéÆ Training Your Chess AI

### **üöÄ Quick Start - First Training Session**

Start with a simple self-play session to test your setup:
```bash
python main.py --games 100 --threads 2
```

### **‚ö° Recommended Training Configuration**

For optimal performance and learning:
```bash
python main.py \
  --games 2000 \
  --threads 6 \
  --difficulty 8 \
  --engine-path "C:/Stockfish/stockfish.exe" \
  --self-play-ratio 0.7 \
  --save-interval 100
```

### **üéØ Training Scenarios**

#### **1. Self-Play Only (Beginner)**
```bash
# Learn basic chess rules and patterns
python main.py --games 1000 --threads 4
```

#### **2. Engine Opposition (Intermediate)**
```bash
# Train against Stockfish at difficulty 5
python main.py --games 1000 --difficulty 5 --engine-path "C:/Stockfish/stockfish.exe"
```

#### **3. Mixed Training (Advanced)**
```bash
# 70% self-play, 30% engine games
python main.py --games 2000 --difficulty 10 --self-play-ratio 0.7 --engine-path "C:/Stockfish/stockfish.exe"
```

#### **4. High-Performance Training**
```bash
# Maximum speed with 8 threads
python main.py --games 5000 --threads 8 --difficulty 12 --engine-path "C:/Stockfish/stockfish.exe"
```

#### **5. Resume from Checkpoint**
```bash
# Continue training from saved model
python main.py --games 1000 --load-model checkpoint_500.pth --difficulty 15 --engine-path "C:/Stockfish/stockfish.exe"
```

### **üñ•Ô∏è Platform-Specific Examples**

**Windows:**
```bash
python main.py --games 1000 --engine-path "C:\Stockfish\stockfish.exe" --threads 6
```

**Linux:**
```bash
python main.py --games 1000 --engine-path "/usr/games/stockfish" --threads 6
```

**macOS:**
```bash
python main.py --games 1000 --engine-path "/opt/homebrew/bin/stockfish" --threads 6
```

## ‚öôÔ∏è Command Line Reference

### **Core Training Parameters**

| Parameter | Type | Default | Description |
|-----------|------|---------|-------------|
| `--games` | int | 1000 | Total number of training games |
| `--threads` | int | 4 | Parallel game threads (1-8 recommended) |
| `--difficulty` | int | 5 | Stockfish skill level (0-20, higher = stronger) |
| `--self-play-ratio` | float | 0.7 | Ratio of self-play vs engine games (0.0-1.0) |

### **Model Management**

| Parameter | Type | Default | Description |
|-----------|------|---------|-------------|
| `--load-model` | str | None | Path to existing model checkpoint |
| `--save-interval` | int | 100 | Save checkpoint every N games |
| `--update-freq` | int | 10 | Update target network every N games |

### **Engine Configuration**

| Parameter | Type | Default | Description |
|-----------|------|---------|-------------|
| `--engine-path` | str | None | Path to Stockfish executable |

### **Parameter Guidelines**

#### **Thread Count Recommendations**
- **1-2 threads**: Testing, debugging, or limited CPU
- **4-6 threads**: Optimal for most systems
- **8+ threads**: High-end systems with 8+ cores

#### **Difficulty Level Guide**
- **0-3**: Beginner level (learns basic moves)
- **4-7**: Intermediate level (develops tactics)
- **8-12**: Advanced level (strategic play)
- **13-20**: Expert level (master-level play)

#### **Self-Play Ratio Strategy**
- **1.0**: Pure self-play (exploration, basic learning)
- **0.7-0.8**: Balanced learning (recommended)
- **0.3-0.5**: Engine-focused (tactical improvement)
- **0.0**: Pure engine opposition (advanced tactics)

## üìä Output & Statistics

During training, you'll see real-time updates:

```
Game  250/1000 üèÜ | W: 85 L: 120 D: 45 | WR: 34.0% | Œµ:0.7750 | Moves: 47 | Speed:3.2g/s | ETA:3.9m

üíæ Checkpoint saved: checkpoint_100.pth
   Current Stats: 85W/120L/45D (34.0% win rate)

üìà Progress Report (250/1000):
   Win Rate: 34.0%
   Epsilon: 0.7750
   Total Reward: 12500
   Time Elapsed: 2.1m
   Avg Speed: 3.15 games/sec
```

**Legend:**
- üèÜ = Win
- üíÄ = Loss  
- ü§ù = Draw
- **WR** = Win Rate
- **Œµ** (epsilon) = Exploration rate (starts at 1.0, decays to 0.01)
- **Speed** = Games per second

## üß† Neural Network Architecture

### **Input Representation**
The neural network processes chess positions using a **12-channel 8√ó8 tensor**:

```
Channel Layout:
‚îú‚îÄ‚îÄ 0-5:  White pieces (Pawn, Knight, Bishop, Rook, Queen, King)
‚îú‚îÄ‚îÄ 6-11: Black pieces (Pawn, Knight, Bishop, Rook, Queen, King)
‚îî‚îÄ‚îÄ Each channel: 8√ó8 binary matrix (1 = piece present, 0 = empty)
```

### **Network Architecture**
```python
ChessNet(
  Conv2d(12 ‚Üí 64,  kernel=3√ó3)  # Spatial feature extraction
  Conv2d(64 ‚Üí 128, kernel=3√ó3)  # Pattern recognition
  Conv2d(128 ‚Üí 128, kernel=3√ó3) # Advanced features
  Linear(8192 ‚Üí 512)            # Position evaluation
  Dropout(0.3)                  # Regularization
  Linear(512 ‚Üí 256)             # Strategic assessment
  Linear(256 ‚Üí 1)               # Q-value output
)
```

### **Key Design Features**
- **Convolutional layers** capture spatial patterns (pawn structures, piece coordination)
- **Dropout regularization** prevents overfitting
- **Target network** stabilizes training with delayed updates
- **Batch processing** evaluates all legal moves simultaneously

## üéØ Training Process Deep Dive

### **Phase 1: Exploration (Games 0-500)**
- **High epsilon (0.8-1.0)**: Random move exploration
- **Self-play focus**: Learn basic chess rules and piece movement
- **Reward shaping**: Penalize illegal moves (-100), reward wins (+1000)
- **Memory building**: Fill experience replay buffer with diverse positions

### **Phase 2: Development (Games 500-2000)**
- **Moderate epsilon (0.3-0.8)**: Balance exploration and exploitation
- **Mixed training**: 70% self-play, 30% engine opposition
- **Tactical learning**: Develop basic tactics and piece coordination
- **Target network updates**: Every 10 games for stable learning

### **Phase 3: Refinement (Games 2000+)**
- **Low epsilon (0.01-0.3)**: Exploit learned strategies
- **Engine opposition**: Challenge against stronger opponents
- **Strategic depth**: Learn opening principles and endgame techniques
- **Progressive difficulty**: Gradually increase Stockfish skill level

### **Reinforcement Learning Components**

#### **Experience Replay**
```python
Memory Buffer (10,000 experiences):
‚îú‚îÄ‚îÄ State: Current board position
‚îú‚îÄ‚îÄ Action: Move taken
‚îú‚îÄ‚îÄ Reward: Game outcome reward
‚îú‚îÄ‚îÄ Next State: Resulting position
‚îî‚îÄ‚îÄ Done: Game termination flag
```

#### **Reward System**
| Outcome | Reward | Purpose |
|---------|--------|---------|
| Win | +1000 | Encourage winning strategies |
| Loss | -1000 | Discourage losing patterns |
| Draw | 0 | Neutral outcome |
| Illegal Move | -100 | Enforce legal play |
| Move Limit | -500 | Encourage efficient play |

#### **Epsilon-Greedy Strategy**
- **Exploration**: Random moves (Œµ probability)
- **Exploitation**: Best predicted moves (1-Œµ probability)
- **Decay**: Œµ decreases from 1.0 ‚Üí 0.01 over training
- **Balance**: Ensures both learning and performance

### **Multi-Threading Architecture**
```
Main Thread:
‚îú‚îÄ‚îÄ Coordinates training
‚îú‚îÄ‚îÄ Updates target network
‚îú‚îÄ‚îÄ Saves checkpoints
‚îî‚îÄ‚îÄ Displays progress

Worker Threads (1-N):
‚îú‚îÄ‚îÄ Play games independently
‚îú‚îÄ‚îÄ Generate experiences
‚îú‚îÄ‚îÄ Update shared memory buffer
‚îî‚îÄ‚îÄ Decay epsilon (thread-safe)

Training Thread:
‚îú‚îÄ‚îÄ Processes experience batches
‚îú‚îÄ‚îÄ Updates neural network
‚îî‚îÄ‚îÄ Maintains thread safety
```

## üîß Advanced Configuration

### Adjust Learning Parameters

Edit `agent.py` to modify:
```python
DQNAgent(
    learning_rate=0.001,    # Learning rate
    gamma=0.99,             # Discount factor
    epsilon=1.0,            # Starting exploration rate
    epsilon_decay=0.9995,   # Exploration decay rate
    epsilon_min=0.01        # Minimum exploration rate
)
```

### Change Memory Size

In `agent.py`:
```python
self.memory = deque(maxlen=10000)  # Replay buffer size
self.batch_size = 64               # Training batch size
```

### Adjust Game Length

In `train.py`:
```python
def play_game(self, env, opponent='self', max_moves=200):
    # Change max_moves to allow longer/shorter games
```

## üíæ Model Files

Training creates checkpoint files:
- `checkpoint_100.pth` - Saved every 100 games (configurable)
- `checkpoint_200.pth`
- ...
- `final_model.pth` - Final trained model

**Load a model:**
```python
agent = DQNAgent()
agent.load('final_model.pth')
```

## üêõ Troubleshooting Guide

### **Installation Issues**

#### **"ModuleNotFoundError: No module named 'stockfish'"**
```bash
# Install the stockfish Python wrapper
pip install stockfish

# Or install from source if needed
pip install git+https://github.com/zhelyabuzhsky/stockfish.git
```

#### **"CUDA out of memory" or GPU not detected**
```bash
# Check CUDA installation
python -c "import torch; print(torch.cuda.is_available())"

# Install CUDA-enabled PyTorch
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118

# If still issues, force CPU mode
export CUDA_VISIBLE_DEVICES=""  # Linux/macOS
set CUDA_VISIBLE_DEVICES=       # Windows
```

### **Engine Connection Problems**

#### **"Engine connection failed" or "Stockfish not found"**
```bash
# Windows - Check path format
python main.py --engine-path "C:/Stockfish/stockfish.exe"

# Linux - Verify installation
which stockfish
python main.py --engine-path "/usr/games/stockfish"

# macOS - Check Homebrew installation
brew list stockfish
python main.py --engine-path "/opt/homebrew/bin/stockfish"

# Test engine manually
stockfish
# Type "quit" to exit
```

#### **"UCI protocol error"**
- Ensure Stockfish version 10+ (older versions may have compatibility issues)
- Try different Stockfish builds (official vs. community builds)
- Use self-play mode as fallback: `python main.py --games 100`

### **Performance Issues**

#### **Training is too slow**
```bash
# Increase thread count (but not more than CPU cores)
python main.py --threads 6 --games 1000

# Enable GPU acceleration
python -c "import torch; print(f'CUDA: {torch.cuda.is_available()}')"

# Reduce game complexity for testing
python main.py --games 100 --threads 2
```

#### **Out of memory errors**
```bash
# Reduce thread count
python main.py --threads 2 --games 1000

# Close other applications
# Reduce batch size in agent.py (line 32): self.batch_size = 32

# Use CPU-only mode if GPU memory is limited
export CUDA_VISIBLE_DEVICES=""
```

#### **Training not progressing (win rate stays low)**
- **Increase training time**: More games needed for complex strategies
- **Adjust difficulty**: Start with lower Stockfish difficulty (1-3)
- **Check epsilon decay**: Should decrease from 1.0 to 0.01 over time
- **Verify reward system**: Check that wins/losses are properly detected

### **Model Loading/Saving Issues**

#### **"Failed to load model"**
```bash
# Check file exists and is readable
ls -la *.pth

# Verify model compatibility
python -c "import torch; print(torch.load('checkpoint_100.pth', map_location='cpu').keys())"

# Start fresh if corrupted
python main.py --games 100  # Creates new model
```

#### **Checkpoint files too large**
- Normal size: ~50MB per checkpoint
- If larger: Check for memory leaks in training loop
- Clean up old checkpoints: `rm checkpoint_*.pth` (keep final_model.pth)

### **Platform-Specific Issues**

#### **Windows**
```bash
# PowerShell execution policy
Set-ExecutionPolicy -ExecutionPolicy RemoteSigned -Scope CurrentUser

# Path issues with backslashes
python main.py --engine-path "C:/Stockfish/stockfish.exe"  # Use forward slashes
```

#### **Linux**
```bash
# Permission issues
sudo chmod +x /usr/games/stockfish

# Missing dependencies
sudo apt install python3-dev build-essential
```

#### **macOS**
```bash
# Homebrew issues
brew update && brew upgrade stockfish

# Python path issues
export PATH="/opt/homebrew/bin:$PATH"
```

### **Getting Help**

If you're still having issues:

1. **Check system requirements**: Ensure Python 3.8+, sufficient RAM, and CPU cores
2. **Verify installation**: Run `python -c "import torch, chess, rich; print('All imports successful')"`
3. **Test minimal setup**: `python main.py --games 10 --threads 1`
4. **Check logs**: Look for error messages in the terminal output
5. **Create issue**: Include your OS, Python version, and full error message

## üìö API Documentation

### **Core Classes**

#### **DQNAgent**
```python
class DQNAgent:
    def __init__(self, learning_rate=0.001, gamma=0.99, epsilon=1.0, 
                 epsilon_decay=0.9995, epsilon_min=0.01):
        """Initialize DQN agent with hyperparameters"""
    
    def select_move(self, env: ChessEnv, fast_mode=True) -> chess.Move:
        """Select move using epsilon-greedy policy"""
    
    def remember(self, state, action, reward, next_state, done):
        """Store experience in replay memory"""
    
    def replay(self) -> float:
        """Train on batch from memory, returns loss"""
    
    def save(self, path: str):
        """Save model checkpoint"""
    
    def load(self, path: str):
        """Load model checkpoint"""
```

#### **ChessEnv**
```python
class ChessEnv:
    def __init__(self):
        """Initialize chess environment"""
    
    def reset(self) -> np.ndarray:
        """Reset to starting position, return initial state"""
    
    def get_state(self) -> np.ndarray:
        """Get current board state as 12√ó8√ó8 tensor"""
    
    def make_move(self, move: chess.Move) -> Tuple[np.ndarray, float, bool]:
        """Make move, return (next_state, reward, done)"""
    
    def get_legal_moves(self) -> List[chess.Move]:
        """Get list of legal moves"""
```

#### **ChessNet**
```python
class ChessNet(nn.Module):
    def __init__(self):
        """Initialize CNN architecture"""
    
    def forward(self, x: torch.Tensor) -> torch.Tensor:
        """Forward pass, returns Q-value"""
```

### **Training Classes**

#### **Trainer**
```python
class Trainer:
    def __init__(self, agent: DQNAgent, use_engine=False, engine_path=None):
        """Initialize trainer with agent and optional engine"""
    
    def play_game(self, env: ChessEnv, opponent='self', max_moves=200) -> dict:
        """Play one game, return result dictionary"""
    
    def train(self, num_games=1000, update_freq=10, save_freq=100, callback=None):
        """Train agent for specified number of games"""
```

#### **UCIEngine**
```python
class UCIEngine:
    def __init__(self, engine_path: str, skill_level: int = 1):
        """Initialize UCI engine interface"""
    
    def set_skill_level(self, level: int):
        """Set engine difficulty (0-20)"""
    
    def get_best_move(self, board: chess.Board) -> chess.Move:
        """Get best move from engine"""
```

## üéØ Training Best Practices

### **Getting Started**
1. **Start small**: Begin with 100-500 games to test your setup
2. **Self-play first**: Let the AI learn basic rules before engine opposition
3. **Monitor progress**: Watch win rate, epsilon decay, and training speed
4. **Save frequently**: Use `--save-interval 50` for frequent checkpoints

### **Progressive Training Strategy**
```bash
# Phase 1: Basic learning (500 games)
python main.py --games 500 --threads 4

# Phase 2: Tactical development (1000 games)
python main.py --games 1000 --difficulty 3 --engine-path "C:/Stockfish/stockfish.exe" --threads 6

# Phase 3: Strategic refinement (2000+ games)
python main.py --games 2000 --difficulty 8 --self-play-ratio 0.6 --engine-path "C:/Stockfish/stockfish.exe" --threads 8
```

### **Performance Optimization**
- **Thread count**: Use 4-6 threads for optimal performance
- **GPU acceleration**: Ensure CUDA is available for faster training
- **Memory management**: Monitor RAM usage, reduce threads if needed
- **Batch processing**: The agent automatically uses batch evaluation for speed

### **Monitoring Training Progress**
- **Win rate**: Should gradually improve from ~20% to 60%+
- **Epsilon decay**: Should decrease from 1.0 to 0.01 over time
- **Training speed**: Monitor games/second for performance
- **Loss convergence**: Training loss should decrease and stabilize

## üìà Performance Benchmarks

### **Training Progress Expectations**

| Training Phase | Games | Win Rate vs Difficulty 5 | Key Learning |
|----------------|-------|---------------------------|--------------|
| **Exploration** | 0-500 | ~20-30% | Basic rules, piece movement |
| **Development** | 500-2000 | ~30-45% | Tactical patterns, piece coordination |
| **Refinement** | 2000-5000 | ~45-60% | Strategic planning, endgames |
| **Mastery** | 5000+ | ~60%+ | Advanced tactics, opening theory |

### **Hardware Performance**

| Configuration | Games/Hour | Memory Usage | Notes |
|---------------|------------|--------------|-------|
| **CPU Only (4 threads)** | ~200-400 | 2-4GB | Basic training |
| **CPU Only (8 threads)** | ~400-800 | 4-8GB | Good performance |
| **GPU + CPU (4 threads)** | ~800-1500 | 4-6GB | Optimal setup |
| **GPU + CPU (8 threads)** | ~1200-2500 | 6-10GB | Maximum performance |

*Note: Performance varies based on hardware, Stockfish difficulty, and game complexity*

## üöÄ Future Enhancements

### **Planned Features**
- [ ] **Policy Gradient Methods** - PPO/A3C for more stable learning
- [ ] **Monte Carlo Tree Search** - MCTS integration for deeper analysis
- [ ] **Opening Book Integration** - Learn from master games
- [ ] **Progressive Difficulty** - Automatic difficulty scaling
- [ ] **Web Interface** - Browser-based training visualization
- [ ] **ELO Rating System** - Track AI strength progression
- [ ] **Multi-GPU Support** - Distributed training across GPUs
- [ ] **Model Comparison** - A/B testing between different architectures

### **Advanced Features**
- [ ] **Endgame Tablebase** - Perfect endgame play
- [ ] **Position Evaluation** - Detailed position analysis
- [ ] **Game Replay** - Visualize training games
- [ ] **Hyperparameter Tuning** - Automated optimization
- [ ] **Model Compression** - Smaller, faster models

## üìÑ License & Usage

This project is released under the **MIT License**. Feel free to:
- ‚úÖ Use for personal and commercial projects
- ‚úÖ Modify and distribute
- ‚úÖ Create derivative works
- ‚úÖ Use for educational purposes

## üôè Acknowledgments

### **Core Technologies**
- **PyTorch** - Deep learning framework
- **python-chess** - Chess library and UCI protocol
- **Stockfish** - Open source chess engine
- **Rich** - Beautiful terminal interfaces

### **Inspiration**
- **AlphaZero** - Self-play reinforcement learning
- **MuZero** - Model-based reinforcement learning
- **Leela Chess Zero** - Open source chess AI
- **DeepMind** - Pioneering AI research

## üìû Support & Community

### **Getting Help**
1. **Read the documentation** - This README covers most common issues
2. **Check troubleshooting** - Comprehensive problem-solving guide
3. **Test minimal setup** - Verify installation with simple commands
4. **Create an issue** - Include system details and error messages

### **Contributing**
We welcome contributions! Areas of interest:
- Performance optimizations
- New training algorithms
- UI/UX improvements
- Documentation enhancements
- Bug fixes and testing

### **Resources**
- **GitHub Repository**: [Your Repository URL]
- **Documentation**: This README
- **Issues**: GitHub Issues page
- **Discussions**: GitHub Discussions (if enabled)

---

## üéâ Ready to Train Your Chess AI?

```bash
# Quick start command
python main.py --games 100 --threads 4

# Full training session
python main.py --games 2000 --threads 6 --difficulty 8 --engine-path "C:/Stockfish/stockfish.exe"
```

**Happy Training! May your neural network achieve chess mastery! ‚ôüÔ∏èüß†üöÄ**

*Remember: Great chess AI takes time to develop. Be patient, monitor progress, and enjoy watching your creation learn and improve!*
